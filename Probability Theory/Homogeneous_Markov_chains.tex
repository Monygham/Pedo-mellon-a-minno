\input ../pree.tex

\begin{document}
\title{Homogeneous Markov chains}
\date{}
\maketitle

\section{General Markov Chains}

\begin{definition}
Let $(\Omega, \cF, P)$ be a probability space and let $\cS$ be a countable set considered as a measurable space with respect to power set $\sigma$-algebra. Suppose that $\{X_n:\Omega\ra \cS\}_{n\in \NN}$ is a sequence of random variables such that
$$P\big(X_{n+1} = s_{n+1}\,\big|\,X_n=s_n,...,X_0=s_0\big) = P\big(X_{n+1}=s_{n+1}\,\big|\,X_n=s_n\big)$$
for all $n\in \NN$ and $s_0,...,s_{n+1}\in \cS$ such that 
$$P\big(X_n=s_n,...,X_0=s_0\big) > 0$$
Then $\{X_n\}_{n\in \NN}$ is \textit{a Markov chain with state space $\cS$}.
\end{definition}

\begin{definition}
Let $\cS$ be a countable set and let $\{p_{ts}\}_{s,t \in \cS}$ be a matrix of nonnegative reals such that
$$\sum_{t\in \cS}p_{ts} = 1$$
for every $s\in \cS$. Then $\{p_{ts}\}_{s,t\in \cS}$ is \textit{a stochastic matrix}.
\end{definition}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$ and fix $n\in \NN$. Suppose that $\{p_{ts}(n)\}_{s,t\in \cS}$ is a stochastic matrix such that
$$p_{ts}(n) = P\big(X_{n+1}=t\,\big|\,X_n=s\big)$$
for every $s,t\in \cS$ such that $P(X_n=s)>0$. Then $\{p_{ts}(n)\}_{s,t\in \cS}$ is called \textit{a transition matrix of $\{X_n\}_{n\in \NN}$ in $n$-th step}.
\end{definition}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Then the distribution of $X_0$ is called \textit{the initial distribution of $\{X_n\}_{n\in \NN}$}.
\end{definition}

\begin{proposition}\label{proposition:markov_chains_are_determined_by_initial_distributions_and_transition_matrices}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Suppose that $\{p_{ts}(n)\}_{s,t\in \cS}$ are transition matrices and $\nu$ is the initial distribution for $\{X_n\}_{n\in \NN}$. Then
$$P\big(X_n = t\big) = \sum_{(s_0,s_1,...,s_{n-1})\in \cS^{n}}p_{ts_{n-1}}(n-1)\cdot p_{s_{n-1}s_{n-2}}(n-2) \cdot ... \cdot p_{s_{1}s_0}(0)\cdot \nu(\{s_0\})$$
for every $t\in \cS$ and $n\in \NN$.
\end{proposition}
\begin{proof}
The proof goes by induction on $n$. The base case is trivial. Indeed, according to the definition of the initial distribution, we have
$$P\big(X_0 = t\big) = \nu(\{t\})$$
for every $t\in \cS$. Suppose that the result holds for some $n\in \NN$. Fix $t\in \cS$. Define
$$\cS_+ = \big\{s\in \cS\,\big|\,P\big(X_n = s\big)>0\big\},\,\cS_0 = \cS\setminus \cS_+$$
Then
$$P\big(X_{n+1} = t\big) = \sum_{s\in \cS}P\big(X_{n+1} = t,X_n=s\big) = \sum_{s\in \cS_+}P\big(X_{n+1} = t,X_n=s\big) + \sum_{s\in \cS_0}P\big(X_{n+1} = t,X_n=s\big) = $$
$$= \sum_{s\in \cS_+}P\big(X_{n+1} = t\,\big|\,X_n=s\big)\cdot P\big(X_n=s\big) = \sum_{s\in \cS_+}P\big(X_{n+1} = t\,\big|\,X_n=s\big)\cdot P\big(X_n=s\big) + \sum_{s\in \cS_0}p_{ts}(n)\cdot P\big(X_n=s\big)=$$
$$=\sum_{s\in \cS_+}p_{ts}(n)\cdot P\big(X_n=s\big) + \sum_{s\in \cS_0}p_{ts}(n)\cdot P\big(X_n=s\big) = \sum_{s\in \cS}p_{ts}(n)\cdot P\big(X_n=s\big)$$
and by inductive assumption we have
$$\sum_{s\in \cS}p_{ts}(n)\cdot P\big(X_n=s\big) = \sum_{s\in \cS} p_{ts}(n)\cdot \left(\sum_{(s_0,...s_{n-1})\in \cS^n} p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})\right) = $$
$$= \sum_{s\in \cS,(s_0,...s_{n-1})\in \cS^n} p_{ts}(n)\cdot p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\}) =$$
$$=\sum_{(s_0,...s_{n-1},s_n)\in \cS^n} p_{ts_n}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})$$
and hence
$$P\big(X_{n+1} = t\big) = \sum_{(s_0,...s_{n-1},s_n)\in \cS^n} p_{ts_n}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})$$
\end{proof}
\noindent
Proposition \ref{proposition:markov_chains_are_determined_by_initial_distributions_and_transition_matrices} shows that Markov chains are determined by initial distributions and transition matrices. The following result establishes inverse fact.



So far we discussed general Markov chains. However, the most interesting situation is given by in the following special case.

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Suppose that there exists a stochastic matrix $\{p_{ts}\}_{s,t\in \cS}$ such that
$$p_{ts} = P\big(X_{n+1} = t\,\big|\,X_n = s\big)$$
for every $s,t\in \cS$ such that $P(X_n=s)>0$. Then $\{X_n\}_{n\in \NN}$ is called \textit{a homogeneous Markov chain}.
\end{definition}

\begin{theorem}\label{theorem:existence_of_Markov_chain_with_given_initial_distribution_and_transition_matrices}
Let $\cS$ be a countable set considered as a measurable space with respect to its power set $\sigma$-algebra. Suppose that $\nu$ is a probability distribution on $\cS$ and $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$ is a sequence of stochastic matrices. Then there exists a probability space $(\Omega,\cF,P)$ and a Markov chain $\{X_n:\Omega\ra \cS\}_{n\in \NN}$ such that $\nu$ is the initial distribution of $\{X_n\}_{n\in \NN}$ and $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$ are its transition matrices.
\end{theorem}
\begin{proof}
Suppose that $[n] = \{0,1,...,n\}$ for $n\in \NN$. Then $\cS^{[n]} = \underbrace{\cS\times ...\times \cS}_{n\,\mathrm{times}}$ together with its power algebra of subsets is a measurable space. We define a probability measure $\mu_n$ in $\cS^{[n]}$ by formula
$$\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})$$
In order to verify that $\mu_n$ is well defined note that
$$\sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = \sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) = $$
$$=\sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS}\sum_{s_n\in \cS}p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) =$$
$$= \sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS}\bigg(\sum_{s_n\in \cS}p_{s_{n}s_{n-1}}(n-1)\bigg)\cdot p_{s_{n-1}s_{n-2}}(n-2)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})= $$
$$=\sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS} p_{s_{n-1}s_{n-2}}(n-2)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})$$
Repeating this simplification $(n-1)$-times more we obtain
$$\sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = \sum_{s_0\in \cS}\nu(\{s_0\}) = 1$$
This proves that $\mu_n$ is well defined. Suppose next that $n_1\leq n_2$. Then we have a projection $\pi_{n_2,n_1}:\cS^{[n_2]}\ra \cS^{[n_1]}$ and
$$\left(\pi_{n_2,n_1}\right)_*\mu_{n_2}\left(\{(s_0,s_1,...,s_{n_1})\}\right) = \mu_{n_2}\left(\pi_{n_2,n_1}^{-1}\left((s_0,s_1,...,s_{n_1})\right)\right) = $$
$$=\sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}\mu_{n_2}\left(\{(s_0,s_1,...,s_{n_1},s_{n_1+1},...,s_{n_2})\}\right) =$$
$$= \sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}p_{s_{n_2}s_{n_2-1}}(n_2-1)\cdot ...\cdot p_{s_{n_1 + 1}s_{n_1}}(n_1)\cdot p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})=$$
$$= \bigg(\sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}p_{s_{n_2}s_{n_2-1}}(n_2-1)\cdot ...\cdot p_{s_{n_1 + 1}s_{n_1}}(n_1)\bigg)\cdot p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) =$$
$$=p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) = \mu_{n_1}\left(\{(s_0,s_1,...,s_{n_1})\}\right)$$
This proves that $\left(\pi_{n_2,n_1}\right)_*\mu_{n_2} = \mu_{n_1}$ for every pair of natural numbers $n_1\leq n_2$. Now suppose that $F$ is a finite subset of $\NN$ and pick $n\in \NN$ such that $F\subseteq [n]$. Next suppose that $\pi_{n,F}:\cS^{[n]}\ra \cS^F$ is the projection on axes parametrized by elements of $F$. We define
$$\mu_F = \left(\pi_{n,F}\right)_*\mu_n$$
Then $\mu_F$ is a probability measure and, since $\left(\pi_{n_2,n_1}\right)_*\mu_{n_2} = \mu_{n_1}$ for every pair of natural numbers $n_1\leq n_2$, we derive that $\mu_F$ does not depend on particular choice of $n\in \NN$ such that $F\subseteq [n]$. It follows that if $F_1\subseteq F_2$ are arbitrary finite subsets of $\NN$ and $\pi_{F_2,F_1}:\cS^{F_2}\ra \cS^{F_1}$ is the projection, then 
$$\left(\pi_{F_2,F_1}\right)_*\mu_{F_2} = \mu_{F_1}$$
Moreover, we have $\mu_n = \mu_{[n]}$ for every $n\in \NN$. Note also that each measure $\mu_F$ is inner regular with respect to discrete topology on $\cS^F$. Therefore, by {\cite[Theorem 2.2]{Daniell_Kolmogorov_extension}} there exists a unique probability measure $\mu$ defined on $\cS^{\NN}$ with $\sigma$-algebra $\cP(\cS)^{\otimes \NN}$ such that
$$\left(\pi_F\right)_*\mu = \mu_F$$
for every finite subset $F$ of $\NN$. We set $\Omega = \cS^{\NN},\cF = \cP(\cS)^{\otimes \NN}$ and $P = \mu$. Then for each $n\in \NN$ we define $X_n:\Omega\ra \cS$ as the projection onto $n$-th axis. We claim that $\{X_n\}_{n\in \NN}$ is a Markov chain with initial distribution $\nu$ and transition matrices $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$. For this fix $s_0,...,s_n,s_{n+1}\in \cS$ for some $n\in \NN$ and suppose that 
$$P\big(X_n=s_n,...,X_0=s_0\big) > 0$$
Since
$$P\big(X_n=s_n,...,X_0=s_0\big) = \mu_n$$
\end{proof}


























\small
\bibliographystyle{apalike}
\bibliography{../zzz}

\end{document}






















\end{document}