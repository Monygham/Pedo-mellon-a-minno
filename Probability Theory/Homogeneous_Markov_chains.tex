\input ../pree.tex

\begin{document}
\title{Homogeneous Markov chains}
\date{}
\maketitle

\section{Introduction}
\noindent
In this notes we study discrete homogeneous Markov chains. We start in the first section with general discrete Markov chains. Main result is the theorem on existence of a Markov chain with given transition matrices and initial distribution. This theorem is basically a consequence of Daniell-Kolmogorov extension theorem proved in \cite{Daniell_Kolmogorov_extension}. In the second section we define homogeneous Markov chains and discuss their basic properties. Next sections are devoted to classification of states of such chains. 

\section{General Markov Chains}

\begin{definition}
Let $(\Omega, \cF, P)$ be a probability space and let $\cS$ be a countable set considered as a measurable space with respect to power set $\sigma$-algebra. Suppose that $\{X_n:\Omega\ra \cS\}_{n\in \NN}$ is a sequence of random variables such that
$$P\big(X_{n+1} = s_{n+1}\,\big|\,X_n=s_n,...,X_0=s_0\big) = P\big(X_{n+1}=s_{n+1}\,\big|\,X_n=s_n\big)$$
for all $n\in \NN$ and $s_0,...,s_{n+1}\in \cS$ such that 
$$P\big(X_n=s_n,...,X_0=s_0\big) > 0$$
Then $\{X_n\}_{n\in \NN}$ is \textit{a Markov chain with state space $\cS$}.
\end{definition}

\begin{definition}
Let $\cS$ be a countable set and let $\{p_{ts}\}_{s,t \in \cS}$ be a matrix of nonnegative reals such that
$$\sum_{t\in \cS}p_{ts} = 1$$
for every $s\in \cS$. Then $\{p_{ts}\}_{s,t\in \cS}$ is \textit{a stochastic matrix}.
\end{definition}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$ and fix $n\in \NN$. Suppose that $\{p_{ts}(n)\}_{s,t\in \cS}$ is a stochastic matrix such that
$$p_{ts}(n) = P\big(X_{n+1}=t\,\big|\,X_n=s\big)$$
for every $s,t\in \cS$ such that $P(X_n=s)>0$. Then $\{p_{ts}(n)\}_{s,t\in \cS}$ is called \textit{a transition matrix of $\{X_n\}_{n\in \NN}$ in $n$-th step}.
\end{definition}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Then the distribution of $X_0$ is called \textit{the initial distribution of $\{X_n\}_{n\in \NN}$}.
\end{definition}

\begin{proposition}\label{proposition:markov_chains_are_determined_by_initial_distributions_and_transition_matrices}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Suppose that $\{p_{ts}(n)\}_{s,t\in \cS}$ are transition matrices and $\nu$ is the initial distribution for $\{X_n\}_{n\in \NN}$. Then
$$P\big(X_n = t\big) = \sum_{(s_0,s_1,...,s_{n-1})\in \cS^{n}}p_{ts_{n-1}}(n-1)\cdot p_{s_{n-1}s_{n-2}}(n-2) \cdot ... \cdot p_{s_{1}s_0}(0)\cdot \nu(\{s_0\})$$
for every $t\in \cS$ and $n\in \NN$.
\end{proposition}
\begin{proof}
The proof goes by induction on $n$. The base case is trivial. Indeed, according to the definition of the initial distribution, we have
$$P\big(X_0 = t\big) = \nu(\{t\})$$
for every $t\in \cS$. Suppose that the result holds for some $n\in \NN$. Fix $t\in \cS$. Define
$$\cS_+ = \big\{s\in \cS\,\big|\,P\big(X_n = s\big)>0\big\},\,\cS_0 = \cS\setminus \cS_+$$
Then
$$P\big(X_{n+1} = t\big) = \sum_{s\in \cS}P\big(X_{n+1} = t,X_n=s\big) = \sum_{s\in \cS_+}P\big(X_{n+1} = t,X_n=s\big) + \sum_{s\in \cS_0}P\big(X_{n+1} = t,X_n=s\big) = $$
$$= \sum_{s\in \cS_+}P\big(X_{n+1} = t\,\big|\,X_n=s\big)\cdot P\big(X_n=s\big) = \sum_{s\in \cS_+}P\big(X_{n+1} = t\,\big|\,X_n=s\big)\cdot P\big(X_n=s\big) + \sum_{s\in \cS_0}p_{ts}(n)\cdot P\big(X_n=s\big)=$$
$$=\sum_{s\in \cS_+}p_{ts}(n)\cdot P\big(X_n=s\big) + \sum_{s\in \cS_0}p_{ts}(n)\cdot P\big(X_n=s\big) = \sum_{s\in \cS}p_{ts}(n)\cdot P\big(X_n=s\big)$$
and by inductive assumption we have
$$\sum_{s\in \cS}p_{ts}(n)\cdot P\big(X_n=s\big) = \sum_{s\in \cS} p_{ts}(n)\cdot \left(\sum_{(s_0,...s_{n-1})\in \cS^n} p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})\right) = $$
$$= \sum_{s\in \cS,(s_0,...s_{n-1})\in \cS^n} p_{ts}(n)\cdot p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\}) =$$
$$=\sum_{(s_0,...s_{n-1},s_n)\in \cS^n} p_{ts_n}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})$$
and hence
$$P\big(X_{n+1} = t\big) = \sum_{(s_0,...s_{n-1},s_n)\in \cS^n} p_{ts_n}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0)\cdot \nu(\{s_0\})$$
\end{proof}
\noindent
Proposition \ref{proposition:markov_chains_are_determined_by_initial_distributions_and_transition_matrices} shows that Markov chains are determined by initial distributions and transition matrices. The following result establishes its converse.

\begin{theorem}\label{theorem:existence_of_Markov_chain_with_given_initial_distribution_and_transition_matrices}
Let $\cS$ be a countable set considered as a measurable space with respect to its power set $\sigma$-algebra. Suppose that $\nu$ is a probability distribution on $\cS$ and $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$ is a sequence of stochastic matrices. Then there exists a probability space $(\Omega,\cF,P)$ and a Markov chain $\{X_n:\Omega\ra \cS\}_{n\in \NN}$ such that $\nu$ is the initial distribution of $\{X_n\}_{n\in \NN}$ and $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$ are its transition matrices.
\end{theorem}
\begin{proof}
Suppose that $[n] = \{0,1,...,n\}$ for $n\in \NN$. Then $\cS^{[n]} = \underbrace{\cS\times ...\times \cS}_{n\,\mathrm{times}}$ together with its power algebra of subsets is a measurable space. We define a probability measure $\mu_n$ in $\cS^{[n]}$ by formula
$$\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})$$
In order to verify that $\mu_n$ is well defined note that
$$\sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = \sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) = $$
$$=\sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS}\sum_{s_n\in \cS}p_{s_{n}s_{n-1}}(n-1)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) =$$
$$= \sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS}\bigg(\sum_{s_n\in \cS}p_{s_{n}s_{n-1}}(n-1)\bigg)\cdot p_{s_{n-1}s_{n-2}}(n-2)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})= $$
$$=\sum_{s_0\in \cS}\sum_{s_1\in \cS}...\sum_{s_{n-2}\in \cS}\sum_{s_{n-1}\in \cS} p_{s_{n-1}s_{n-2}}(n-2)\cdot ... \cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})$$
Repeating this simplification $(n-1)$-times more we obtain
$$\sum_{(s_0,s_1,...,s_n)\in \cS^{[n]}}\mu_n\left(\{(s_0,s_1,...,s_n)\}\right) = \sum_{s_0\in \cS}\nu(\{s_0\}) = 1$$
This proves that $\mu_n$ is well defined. Suppose next that $n_1\leq n_2$. Then we have a projection $\pi_{n_2,n_1}:\cS^{[n_2]}\ra \cS^{[n_1]}$ and
$$\left(\pi_{n_2,n_1}\right)_*\mu_{n_2}\left(\{(s_0,s_1,...,s_{n_1})\}\right) = \mu_{n_2}\left(\pi_{n_2,n_1}^{-1}\left((s_0,s_1,...,s_{n_1})\right)\right) = $$
$$=\sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}\mu_{n_2}\left(\{(s_0,s_1,...,s_{n_1},s_{n_1+1},...,s_{n_2})\}\right) =$$
$$= \sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}p_{s_{n_2}s_{n_2-1}}(n_2-1)\cdot ...\cdot p_{s_{n_1 + 1}s_{n_1}}(n_1)\cdot p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\})=$$
$$= \bigg(\sum_{s_{n_1+1}\in \cS}...\sum_{s_{n_2}\in \cS}p_{s_{n_2}s_{n_2-1}}(n_2-1)\cdot ...\cdot p_{s_{n_1 + 1}s_{n_1}}(n_1)\bigg)\cdot p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) =$$
$$=p_{s_{n_1}s_{n_1-1}}(n_1-1) \cdot ...\cdot p_{s_{1}s_{0}}(0)\cdot \nu(\{s_0\}) = \mu_{n_1}\left(\{(s_0,s_1,...,s_{n_1})\}\right)$$
This proves that $\left(\pi_{n_2,n_1}\right)_*\mu_{n_2} = \mu_{n_1}$ for every pair of natural numbers $n_1\leq n_2$. Now suppose that $F$ is a finite subset of $\NN$ and pick $n\in \NN$ such that $F\subseteq [n]$. Next suppose that $\pi_{n,F}:\cS^{[n]}\ra \cS^F$ is the projection on axes parametrized by elements of $F$. We define
$$\mu_F = \left(\pi_{n,F}\right)_*\mu_n$$
Then $\mu_F$ is a probability measure and, since $\left(\pi_{n_2,n_1}\right)_*\mu_{n_2} = \mu_{n_1}$ for every pair of natural numbers $n_1\leq n_2$, we derive that $\mu_F$ does not depend on particular choice of $n\in \NN$ such that $F\subseteq [n]$. It follows that if $F_1\subseteq F_2$ are arbitrary finite subsets of $\NN$ and $\pi_{F_2,F_1}:\cS^{F_2}\ra \cS^{F_1}$ is the projection, then 
$$\left(\pi_{F_2,F_1}\right)_*\mu_{F_2} = \mu_{F_1}$$
Moreover, we have $\mu_n = \mu_{[n]}$ for every $n\in \NN$. Note also that each measure $\mu_F$ is inner regular with respect to discrete topology on $\cS^F$. Therefore, by {\cite[Theorem 2.2]{Daniell_Kolmogorov_extension}} there exists a unique probability measure $\mu$ defined on $\cS^{\NN}$ with $\sigma$-algebra $\cP(\cS)^{\otimes \NN}$ such that
$$\left(\pi_F\right)_*\mu = \mu_F$$
for every finite subset $F$ of $\NN$. We set $\Omega = \cS^{\NN},\cF = \cP(\cS)^{\otimes \NN}$ and $P = \mu$. Then for each $n\in \NN$ we define $X_n:\Omega\ra \cS$ as the projection onto $n$-th axis. We describe now joint distribution of $(X_0,...,X_n)$ for some $n\in \NN$. For this fix $s_0,...,s_n \in \cS$ and note that
$$P\big(X_n=s_n,...,X_0=s_0\big) = \mu\left(\{(s_0,s_1,...,s_n)\}\times \cS^{\NN\setminus [n]}\right) = \left(\pi_{[n]}\right)_*\mu\left(\{(s_0,s_1,...,s_n)\}\right) =$$
$$= \mu_n\left(\{(s_0,...,s_n)\}\right) = p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})$$
In addition for $n\in \NN$ and $t,s\in \cS$ we have formulas
It follows that for $n\in \NN$ and $t\in \cS$ we have
$$P(X_n = t) = \mu\left(\cS^{[n-1]}\times \{t\}\times \cS^{\NN\setminus [n]}\right) = \left(\pi_{[n]}\right)_*\mu\left(\cS^{[n-1]}\times \{t\}\right) =$$
$$= \mu_n\left(\cS^{[n-1]}\times \{t\}\right) = \sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{ts_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})$$
and
$$P(X_{n+1} = t,X_n=s) = \mu\left(\cS^{[n-1]}\times \{(s,t)\}\times \cS^{\NN\setminus [n]}\right) = \left(\pi_{[n]}\right)_*\mu\left(\cS^{[n-1]}\times \{(s,t)\}\right) =$$
$$= \mu_n\left(\cS^{[n-1]}\times \{(s,t)\}\right) = \sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{ts}(n)\cdot p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})$$
We claim that $\{X_n\}_{n\in \NN}$ is a Markov chain with initial distribution $\nu$ and transition matrices $\{p_{ts}(n)\}_{s,t\in \cS}$ for $n\in \NN$. In order to prove the claim we use the description of joint distributions $(X_1,...,X_n)$ and distribution of $X_n$ for every $n\in \NN$. For $n\in \NN$ and $s_0,...,s_n,s_{n+1}\in \cS$ such that
$$P\big(X_n=s_n,...,X_0=s_0\big) > 0$$
we have
$$P\left(X_{n+1}=s_{n+1}\,|\,X_n=s_n,...,X_0=s_0\right) = \frac{P\big(X_{n+1}=s_{n+1},X_n=s_n,...,X_0=s_0\big)}{P\big(X_n=s_n,...,X_0=s_0\big)} =$$
$$= \frac{p_{s_{n+1}s_n}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})}{p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})} = p_{s_{n+1}s_n}(n)$$
On the other hand we have
$$P\left(X_{n+1}=s_{n+1}\,|\,X_n=s_n\right) = \frac{P\big(X_{n+1}=s_{n+1},X_n=s_n\big)}{P\big(X_n=s_n\big)} = $$
$$=\frac{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{s_{n+1}s_{n}}(n)\cdot p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})}{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})} =$$
$$= \frac{p_{s_{n+1}s_{n}}(n)\cdot \left(\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}} p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})\right)}{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{s_ns_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})} = p_{s_{n+1}s_n}(n)$$
Therefore, we have
$$P\left(X_{n+1}=s_{n+1}\,|\,X_n=s_n,...,X_0=s_0\right) = P\left(X_{n+1}=s_{n+1}\,|\,X_n=s_n\right)$$
Thus $\{X_n\}_{n\in \NN}$ is a Markov chain. Moreover, if $t,s\in \cS$ and $n\in \NN$ are such that
$$P(X_n = s) > 0$$
then we have
$$P\left(X_{n+1}=t\,|\,X_n=s\right) = \frac{P\big(X_{n+1}=t,X_n=s\big)}{P\big(X_n=s\big)} = $$
$$=\frac{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{ts}(n)\cdot p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})}{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})} =$$
$$= \frac{p_{ts}(n)\cdot \left(\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}} p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})\right)}{\sum_{(s_0,s_1,...,s_{n-1})\in \cS^{[n-1]}}p_{ss_{n-1}}(n-1)\cdot ... \cdot p_{s_1s_0}(0) \cdot \nu(\{s_0\})} = p_{ts}(n)$$
and we have
$$P(X_0 = s) = \nu(\{s\})$$
This completes the proof of the claim. Hence the theorem is proved.
\end{proof}
    
\section{Homogeneous Markov chains and their state spaces}
\noindent
So far we discussed general Markov chains. However, the following special case is very important.

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a Markov chain with state space $\cS$. Suppose that there exists a stochastic matrix $\{p_{ts}\}_{s,t\in \cS}$ such that
$$p_{ts} = P\big(X_{n+1} = t\,\big|\,X_n = s\big)$$
for every $s,t\in \cS$ such that $P(X_n = s)>0$. Then $\{X_n\}_{n\in \NN}$ is \textit{a homogeneous Markov chain}.
\end{definition}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a homogeneous Markov chain with state space $\cS$ and transition matrix $P$. A state $t$ is \textit{accessible} from state $s$ for $s,t\in \cS$ if there exists $n \in \NN_+$ such that 
$$\left(P^n\right)_{ts} > 0$$
If $t$ is accessible from $s$, then we write $s\ra t$.
\end{definition}

\begin{fact}\label{fact:accessibility_is_transitive}
Let $\{X_n\}_{n\in \NN}$ be a homogeneous Markov chain with state space $\cS$. If $s_1\ra s_2$ and $s_2\ra s_3$ for some $s_1,s_2,s_3\in \cS$, then also $s_1\ra s_3$.
\end{fact}
\begin{proof}
Let $P$ be a transition matrix of $\{X_n\}_{n\in \NN}$. By assumptions
$$\left(P^{n_2}\right)_{s_3s_2}>0,\,\left(P^{n_1}\right)_{s_2s_1}>0,
$$
for some $n_1,n_2\in \NN_+$. Hence
$$\left(P^{n_1+n_2}\right)_{s_3s_1} \geq \left(P^{n_2}\right)_{s_3s_2}\cdot \left(P^{n_1}\right)_{s_2s_1} > 0$$
Thus $s_1\ra s_3$.
\end{proof}

\begin{definition}
Let $\{X_n\}_{n\in \NN}$ be a homogeneous Markov chain with state space $\cS$ and transition matrix $P$. Consider a subset $C$ of $\cS$. Suppose that for every $t\in \cS$ if $s\ra t$ for $s\in C$ , then $t\in C$. Then $C$ is \textit{a closed set of states}.
\end{definition}

% \begin{fact}
% Let $\{X_n\}_{n\in \NN}$ be a homogeneous Markov chain with state space $\cS$ and transition matrix $P$. Suppose that a subset $C$ of $\cS$ is closed. Then 
% \end{fact}
\noindent
Let $\{X_n\}_{n\in \NN}$ be a homogeneous Markov chain with state space $\cS$ and transition matrix $P$. Consider states $s,t$ in $\cS$ and define
$$F_{ts} = \sum_{n=1}^{+\infty}\left(P^n\right)_{ts}$$

\begin{definition}

\end{definition}































\small
\bibliographystyle{apalike}
\bibliography{../zzz}

\end{document}